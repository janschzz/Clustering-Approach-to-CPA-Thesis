{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "606160b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Set, Tuple, List, Union, Callable\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "from sklearn.cluster import AgglomerativeClustering, Birch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab97624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframes_from_folder(folder_path: str, file_limit: int) -> List[Tuple[pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Read dataframes from CSV files in the given folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        file_limit (int): Maximum number of files to be read.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[pd.DataFrame, str]]: A list of tuples containing the dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    tuple_dataframes = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            if len(tuple_dataframes) < file_limit:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                df = pd.read_csv(file_path)\n",
    "                tuple_dataframes.append((df, os.path.basename(file_path)))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return tuple_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bccdf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframes(tuple_dataframes: List[Tuple[pd.DataFrame, str]]) -> None:\n",
    "    \"\"\"\n",
    "    Clean the dataframes by renaming columns, dropping the 'Unnamed: 0' column, and dropping empty columns.\n",
    "\n",
    "    Args:\n",
    "        tuple_dataframes (List[Tuple[pd.DataFrame, str]]): List of tuples containing the dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # inner function\n",
    "    def drop_empty_columns_in_dataframes(dataframes: List[Tuple[pd.DataFrame, str]]) -> None:\n",
    "        for df, _ in dataframes:\n",
    "            empty_columns = df.columns[df.isnull().all()].tolist()  # get the list of empty columns\n",
    "            df.drop(empty_columns, axis=1, inplace=True)  # drop the empty columns\n",
    "    \n",
    "    # renaming columns, deleting 'Unnamed: 0', and removing empty dataframes\n",
    "    indices_to_delete = []\n",
    "    for index, (df, _) in enumerate(tuple_dataframes):\n",
    "        df.rename(columns=lambda x: x.replace('col', ''), inplace=True)\n",
    "        \n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df.drop('Unnamed: 0', axis=1, inplace=True)  # delete the \"Unnamed: 0\" column\n",
    "        \n",
    "        if df.empty:\n",
    "            indices_to_delete.append(index)  # mark dataframe for deletion if empty\n",
    "    \n",
    "    # reverse sort the indices and delete the empty dataframes\n",
    "    for index in sorted(indices_to_delete, reverse=True):\n",
    "        del tuple_dataframes[index]\n",
    "\n",
    "    # call inner funtion\n",
    "    drop_empty_columns_in_dataframes(tuple_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce4f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_without_label(dataframes: List[Tuple[pd.DataFrame, str]], label_df: pd.DataFrame) -> List[Tuple[pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Drop columns for which no gold standard exists.\n",
    "\n",
    "    Args:\n",
    "        dataframes (List[Tuple[pd.DataFrame, str]]): List of tuples containing the dataframes and their corresponding filenames.\n",
    "        label_df (pd.DataFrame): DataFrame containing the gold standard annotations.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[pd.DataFrame, str]]: List of tuples containing the cleaned dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # make a copy\n",
    "    tuple_dataframes_copy = [(df.copy(), name) for df, name in dataframes]\n",
    "\n",
    "    # indices of dataframes to delete\n",
    "    to_delete = []\n",
    "\n",
    "    # iterate over dataframes\n",
    "    for table_index, (df, filename) in enumerate(tuple_dataframes_copy):\n",
    "        for column_index, col in enumerate(df.columns):\n",
    "            if not any((label_df['table_id'] == filename[:-4] + \"_dbpedia\") & (label_df['target_column'] == column_index)):\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "        if df.empty:\n",
    "            to_delete.append(table_index)\n",
    "        else:\n",
    "            tuple_dataframes_copy[table_index] = (df, filename)\n",
    "\n",
    "    # delete empty dataframes from the list in reverse order to prevent index shifting\n",
    "    for index in reversed(to_delete):\n",
    "        del tuple_dataframes_copy[index]\n",
    "\n",
    "    return tuple_dataframes_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86016b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_columns(tuple_dataframes: List[Tuple[pd.DataFrame, str]], n: int) -> List[Tuple[pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Sample 'n' columns from all DataFrames combined, delete non-sampled columns, and filter out empty DataFrames.\n",
    "\n",
    "    Args:\n",
    "        tuple_dataframes (List[Tuple[pd.DataFrame, str]]): A list of tuples containing the dataframes and their corresponding filenames.\n",
    "        n (int): Number of columns to sample in total.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[pd.DataFrame, str]]: A list of tuples containing the non-empty dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    # add prefixes to ensure unique column names in combined DataFrame\n",
    "    all_dfs = []\n",
    "    for idx, (df, _) in enumerate(tuple_dataframes):\n",
    "        df_prefixed = df.add_prefix(f\"df{idx}_\")\n",
    "        all_dfs.append(df_prefixed)\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, axis=1)\n",
    "\n",
    "    # sample 'n' columns from the combined DataFrame\n",
    "    if n >= len(combined_df.columns):\n",
    "        sampled_columns = combined_df.columns\n",
    "    else:\n",
    "        sampled_columns = random.sample(list(combined_df.columns), n)\n",
    "\n",
    "    # filter combined DataFrame to keep only the sampled columns\n",
    "    sampled_combined_df = combined_df[sampled_columns]\n",
    "\n",
    "    filtered_dataframes = []\n",
    "\n",
    "    # split the combined DataFrame back into individual DataFrames and filter out any empty DataFrames\n",
    "    for idx, (_, filename) in enumerate(tuple_dataframes):\n",
    "        relevant_cols = [col for col in sampled_combined_df.columns if col.startswith(f\"df{idx}_\")]\n",
    "        df_sampled = sampled_combined_df[relevant_cols].rename(columns=lambda x: x.split(\"_\", 1)[1])\n",
    "        if not df_sampled.empty:\n",
    "            filtered_dataframes.append((df_sampled, filename))\n",
    "\n",
    "    return filtered_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250afd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_map(label_df: pd.DataFrame, tuple_dataframes: List[Tuple[pd.DataFrame, str]]) -> Dict[str, Set[int]]:\n",
    "    \"\"\"\n",
    "    Create a ground truth map for evaluation.\n",
    "\n",
    "    Args:\n",
    "        label_df (pd.DataFrame): DataFrame containing true labels for evaluation.\n",
    "        tuple_dataframes (List[Tuple[pd.DataFrame, str]]): List of tuples containing the dataframes and their corresponding filenames.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Set[int]]: A dictionary mapping label names to sets of global indices that belong to each label.\n",
    "    \"\"\"\n",
    "    ground_truth = defaultdict(set)\n",
    "\n",
    "    for table_index, (df, filename) in enumerate(tuple_dataframes):\n",
    "        for new_col_index, old_col_index in enumerate(df.columns):\n",
    "            label = label_df.query(f\"table_id == '{filename[:-4] + '_dbpedia'}' and target_column == {old_col_index}\")['annotation_label']\n",
    "\n",
    "            if label.empty:\n",
    "                continue\n",
    "\n",
    "            global_index = sum(df.shape[1] for df, _ in tuple_dataframes[:table_index]) + new_col_index\n",
    "            ground_truth[label.iloc[0]].add(global_index)\n",
    "\n",
    "            if len(label.values) != 1:\n",
    "                raise ValueError(\"Unexpected number of label values\")\n",
    "\n",
    "    for tuple_dataframe in tuple_dataframes:\n",
    "        tuple_dataframe[0].columns = range(len(tuple_dataframe[0].columns))\n",
    "\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3a80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustering(distance_matrix: np.ndarray, n_clusters: int) -> Dict[int, Set[int]]:\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on the distance matrix.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (np.ndarray): Pairwise distance matrix.\n",
    "        n_clusters (int): Number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Set[int]]: A dictionary mapping cluster labels to sets of global indices that belong to each cluster.\n",
    "    \"\"\"\n",
    "    # create an AgglomerativeClustering model\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage='average')\n",
    "\n",
    "    # fit the model to the distance matrix\n",
    "    model.fit(distance_matrix)\n",
    "\n",
    "    # get the cluster labels\n",
    "    labels = model.labels_\n",
    "\n",
    "    # create a dictionary that maps each cluster label to the set of indices that belong to that cluster\n",
    "    clusters = defaultdict(set)\n",
    "    for idx, label in enumerate(labels):\n",
    "        clusters[label].add(idx)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd635296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_micro(truth_dict: Dict[str, Set[int]], result_dict: Dict[int, Set[int]]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance using micro-average precision, recall.\n",
    "\n",
    "    Args:\n",
    "        truth_dict (Dict[str, Set[int]]): Ground truth mapping label names to sets of global indices.\n",
    "        result_dict (Dict[int, Set[int]]): Resulting clustering mapping cluster labels to sets of indices.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Micro-average precision and recall.\n",
    "    \"\"\"\n",
    "    # creating the contingency matrix\n",
    "    contingency_matrix = np.zeros((len(truth_dict), len(result_dict)))\n",
    "\n",
    "    truth_labels = list(truth_dict.keys())\n",
    "    result_labels = list(result_dict.keys())\n",
    "\n",
    "    for i, truth_label in enumerate(truth_labels):\n",
    "        for j, result_label in enumerate(result_labels):\n",
    "            # length of intersection between clusters\n",
    "            # contingency_matrix[i, j] is the number of (global) indices assigned to both clusters\n",
    "            contingency_matrix[i, j] = len(truth_dict[truth_label] & result_dict[result_label])\n",
    "\n",
    "    # calculate precision, recall, and f1-score for each label and average them\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for i, _ in enumerate(result_labels):\n",
    "        tp = np.max(contingency_matrix[:, i])  # maximum element in column i (True Positives)\n",
    "        best_match_index = np.argmax(contingency_matrix[:, i])  # get the index of the best match (TP - index)\n",
    "        tp_plus_fp = np.sum(contingency_matrix[best_match_index, :])  # sum of elements in row of best match (TP + FP)\n",
    "        tp_plus_fn = np.sum(contingency_matrix[:, i])  # sum of elements in column i (TP + FN)\n",
    "\n",
    "        weight = tp_plus_fn  # the weight for each label is the total number of true positives plus false negatives\n",
    "\n",
    "        if tp_plus_fp > 0:\n",
    "            precision = tp / tp_plus_fp\n",
    "            total_precision += weight * precision\n",
    "        if tp_plus_fn > 0:\n",
    "            recall = tp / tp_plus_fn\n",
    "            total_recall += weight * recall\n",
    "\n",
    "        total_weight += weight\n",
    "\n",
    "    avg_precision = total_precision / total_weight\n",
    "    avg_recall = total_recall / total_weight\n",
    "\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f126a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(emb1: List[float], emb2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cosine distance between two embeddings.\n",
    "\n",
    "    Args:\n",
    "        emb1 (List[float]): First embedding.\n",
    "        emb2 (List[float]): Second embedding.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine distance between the two embeddings. Ranges between 0 (identical) and 2 (completely opposite).\n",
    "    \"\"\"\n",
    "    return 1 - np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faf376ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Number of Entries\n",
    "def number_of_entries(column: pd.Series) -> int:\n",
    "    return len(column)\n",
    "\n",
    "# 2. Maximum Entry Length\n",
    "def max_entry_length(column: pd.Series) -> int:\n",
    "    return column.astype(str).str.len().max()\n",
    "\n",
    "# 3. Average Alphabetic Characters per Entry\n",
    "def avg_alpha_characters(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: sum(c.isalpha() for c in str(x))).mean()\n",
    "\n",
    "# 4. Proportion of Entries Containing Numbers\n",
    "def proportion_of_numeric_entries(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: str(x).replace(\".\", \"\", 1).isdigit()).mean()\n",
    "\n",
    "# 5. Column Entropy\n",
    "def column_entropy(column: pd.Series) -> float:\n",
    "    value_counts = column.value_counts(normalize=True)\n",
    "    return -np.sum(value_counts * np.log2(value_counts))\n",
    "\n",
    "# 6. Proportion of Entries with Letters\n",
    "def proportion_of_entries_with_letters(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: any(c.isalpha() for c in x)).mean()\n",
    "\n",
    "# 7. Number of Empty Entries\n",
    "def number_of_empty_entries(column: pd.Series) -> int:\n",
    "    return column.isna().sum()\n",
    "\n",
    "# 8. Average Length of Entries\n",
    "def average_length_of_entries(column: pd.Series) -> float:\n",
    "    return column.astype(str).str.len().mean()\n",
    "\n",
    "# 9. Proportion of Distinct Values\n",
    "def ratio_of_unique_entries(column: pd.Series) -> float:\n",
    "    return column.nunique() / len(column)\n",
    "\n",
    "# 10. Average Number of Numerical Characters per Entry\n",
    "def avg_number_of_numerical_characters(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: sum(c.isdigit() for c in str(x))).mean()\n",
    "\n",
    "# 11-13. Mean, Median and Standard Deviation of Numeric Entries\n",
    "def get_numeric_entries(column: pd.Series) -> pd.Series:\n",
    "    numeric_entries = column.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    return numeric_entries[numeric_entries.notna()]\n",
    "\n",
    "def mean_of_numeric_entries(column: pd.Series) -> float:\n",
    "    numeric_entries = get_numeric_entries(column)\n",
    "    return numeric_entries.mean()\n",
    "\n",
    "def median_of_numeric_entries(column: pd.Series) -> float:\n",
    "    numeric_entries = get_numeric_entries(column)\n",
    "    return numeric_entries.median()\n",
    "\n",
    "def std_dev_of_numeric_entries(column: pd.Series) -> float:\n",
    "    numeric_entries = get_numeric_entries(column)\n",
    "    return numeric_entries.std()\n",
    "\n",
    "# 14. Proportion of Alphanumerical Entries\n",
    "def proportion_of_alphanumeric_entries(column: pd.Series) -> float:\n",
    "    alphanumeric_pattern = re.compile(r'\\w')\n",
    "    return column.astype(str).apply(lambda x: bool(alphanumeric_pattern.match(x))).mean()\n",
    "\n",
    "# 15. Proportion of Numeric-only Entries\n",
    "def proportion_of_integer_entries(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: str(x).isdigit()).mean()\n",
    "\n",
    "# 16. Proportion of Data Entries\n",
    "def proportion_of_date_entries(column: pd.Series) -> float:\n",
    "    date_pattern = re.compile(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\\b')\n",
    "    return column.astype(str).apply(lambda x: bool(date_pattern.match(x))).mean()\n",
    "\n",
    "# 17. Proportion of Entries with Special Characters\n",
    "def proportion_of_entries_with_special_characters(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: any(not c.isalnum() and not c.isspace() for c in x)).mean()\n",
    "\n",
    "# 18. Proportion of Entries Containing Spaces\n",
    "def proportion_of_entries_with_spaces(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: ' ' in x).mean()\n",
    "\n",
    "# 19. Proportion of Link Entries\n",
    "def proportion_of_link_entries(column: pd.Series) -> float:\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return column.astype(str).apply(lambda x: bool(url_pattern.search(x))).mean()\n",
    "\n",
    "# 20. Proportion of Entries Initiating with Capital Letters\n",
    "def proportion_of_capitalized_entries(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: x[0].isupper() if x else False).mean()\n",
    "\n",
    "# 21. Proportion of Entries in Uppercase\n",
    "def proportion_of_fully_capitalized_entries(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: x.isupper()).mean()\n",
    "\n",
    "# 22. Proportion of Words Starting with a Capital Letter\n",
    "def proportion_of_capitalized_words(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: sum(1 for word in x.split() if word.istitle())).mean()\n",
    "\n",
    "feature_extractors = {\n",
    "    \"Number of Entries\": number_of_entries,\n",
    "    \"Maximum Entry Length\": max_entry_length,\n",
    "    \"Average Alphabetic Characters per Entry\": avg_alpha_characters,\n",
    "    \"Proportion of Entries Containing Numbers\": proportion_of_numeric_entries,\n",
    "    \"Column Entropy\": column_entropy,\n",
    "    \"Proportion of Entries with Letters\": proportion_of_entries_with_letters,\n",
    "    \"Number of Empty Entries\": number_of_empty_entries,\n",
    "    \"Average Length of Entries\": average_length_of_entries,\n",
    "    \"Proportion of Distinct Values\": ratio_of_unique_entries,\n",
    "    \"Average Number of Numerical Characters per Entry\": avg_number_of_numerical_characters,\n",
    "    \"Mean Numeric Value\": mean_of_numeric_entries,\n",
    "    \"Median of Numeric Values\": median_of_numeric_entries,\n",
    "    \"Spread of Numeric Values (Standard Deviation)\": std_dev_of_numeric_entries,\n",
    "    \"Proportion of Alphanumerical Entries\": proportion_of_alphanumeric_entries,\n",
    "    \"Proportion of Numeric-only Entries\": proportion_of_integer_entries,\n",
    "    \"Proportion of Data Entries\": proportion_of_date_entries,\n",
    "    \"Proportion of Entries with Special Characters\": proportion_of_entries_with_special_characters,\n",
    "    \"Proportion of Entries Containing Spaces\": proportion_of_entries_with_spaces,\n",
    "    \"Proportion of Link Entries\": proportion_of_link_entries,\n",
    "    \"Proportion of Entries Initiating with Capital Letters\": proportion_of_capitalized_entries,\n",
    "    \"Proportion of Entries in Uppercase\": proportion_of_fully_capitalized_entries,\n",
    "    \"Proportion of Words Starting with a Capital Letter\": proportion_of_capitalized_words\n",
    "}\n",
    "\n",
    "def extract_features(dataframes: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features\n",
    "    \n",
    "    Args:\n",
    "       dataframes (list): data from which features are extracted.\n",
    "       \n",
    "    Returs:\n",
    "        pd.DataFrame: feature dataframe\n",
    "    \"\"\"\n",
    "    # initialize an empty DataFrame to store the features\n",
    "    features = pd.DataFrame(columns=feature_extractors.keys())\n",
    "    \n",
    "    for df in dataframes:\n",
    "        \n",
    "        # dataframe for features of current df\n",
    "        temp_features = pd.DataFrame(columns=feature_extractors.keys())  \n",
    "        \n",
    "        for column_name, column_data in df.items():\n",
    "            \n",
    "            # convert column_data to a pd.Series of strings\n",
    "            column_data_series = pd.Series(map(str, column_data))\n",
    "            \n",
    "            for feature_name, feature_extractor in feature_extractors.items():\n",
    "                \n",
    "                # loc at (column_name, feature_name)\n",
    "                temp_features.loc[column_name, feature_name] = feature_extractor(column_data_series)\n",
    "                \n",
    "        # rowwise concatenation        \n",
    "        features = pd.concat([features, temp_features]) \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83264145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalize_features(feature_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Z-normalize a set of features.\n",
    "\n",
    "    Args:\n",
    "        feature_df (pd.DataFrame): Extracted features\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The z-normalized features.\n",
    "    \"\"\"\n",
    "    # compute the mean and standard deviation for each feature (column).\n",
    "    means = np.mean(feature_df, axis=0)\n",
    "    stds = np.std(feature_df, axis=0)\n",
    "\n",
    "    # z-normalize each feature in the feature set.\n",
    "    z_normalized_features = (feature_df - means) / stds\n",
    "\n",
    "    return z_normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9d6ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_clustering(feature_df: pd.DataFrame, n_clusters: int) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Pre-Clustering using Birch.\n",
    "    \n",
    "    Args:\n",
    "        feature_df (pd.DataFrame): Z-normalized features.\n",
    "        n_clusters (int): number of pre-clusters.\n",
    "        \n",
    "    Returns: \n",
    "        \n",
    "        Dict[int, List[int]]: Dict[int, Set[int]]: A dictionary mapping cluster labels to sets of global indices that belong to each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializing Birch and fit it to the data\n",
    "    birch_model = Birch(n_clusters=n_clusters)\n",
    "    birch_model.fit(feature_df)\n",
    "\n",
    "    # get the cluster labels\n",
    "    labels = birch_model.labels_\n",
    "\n",
    "    # create a dictionary that maps each cluster label to a list of column indices\n",
    "    cluster_to_columns = defaultdict(list)\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        cluster_to_columns[label].append(idx) # globale index\n",
    "\n",
    "    return dict(cluster_to_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7bafd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_column_similarity(cluster: List[List], s) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    inner function\n",
    "    \"\"\"\n",
    "    # convert cluster columns to dataframes for uniform processing\n",
    "    dfs = [pd.DataFrame(col) for col in cluster]\n",
    "    \n",
    "    column_embeddings = {}\n",
    "    count = 0\n",
    "    for df in dfs:\n",
    "        for col in df.columns:\n",
    "            # sample a subset of entries from the column\n",
    "            samples = df[col].dropna().astype(str)\n",
    "            samples = samples.sample(min(len(samples), s))\n",
    "\n",
    "            # Embed\n",
    "            sample_embeddings = model.encode(samples.tolist())\n",
    "            column_embeddings[count] = np.mean(sample_embeddings, axis=0)\n",
    "            count += 1\n",
    "\n",
    "    # compute the distance\n",
    "    embedding_matrix = np.vstack(list(column_embeddings.values()))\n",
    "    distance = 1 - cosine_similarity(embedding_matrix)\n",
    "    \n",
    "    return distance\n",
    "        \n",
    "    \n",
    "def find_centroid(cluster: List[List], s) -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    inner function\n",
    "    \"\"\"\n",
    "    distance_matrix = calculate_column_similarity(cluster, s)\n",
    "    centroid_intern_index = np.argmin(np.sum(distance_matrix, axis=0))\n",
    "    return centroid_intern_index, distance_matrix\n",
    "\n",
    "def compute_cluster_centroids(dataframes: List[pd.DataFrame],\n",
    "                              cluster_to_columns: Dict[int, List[int]],\n",
    "                              s: int,\n",
    "                              index_to_tuple: Callable[[int], Tuple[int, int]]) -> Dict[int, Tuple[int, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute the (pre-)cluster centroids and saves internal distance matrices \n",
    "\n",
    "    Args:\n",
    "        dataframes (List[pd.DataFrame]): List of dataframes containing the data.\n",
    "        cluster_to_columns (Dict[int, List[int]]): Mapping of global cluster IDs to lists of global column indices.\n",
    "        s (int): sampling parameter\n",
    "        index_to_tuple (Callable[[int], Tuple[int, int]]): global to local index map\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Tuple[int, np.ndarray]]: Dictionary where keys are cluster IDs and values are tuples consisting of:\n",
    "            - The global index of the centroid column.\n",
    "            - A numpy array (presumed to be a distance matrix for the centroid).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    centroids = {}\n",
    "\n",
    "    for cluster_id, column_indices in cluster_to_columns.items():\n",
    "        cluster_columns = [dataframes[df_index].iloc[:, col_index].tolist() for df_index, col_index in map(index_to_tuple, column_indices)]\n",
    "        centroid_intern_index, distance_matrix = find_centroid(cluster_columns, s)\n",
    "        centroid_global_index = column_indices[centroid_intern_index]\n",
    "        centroids[cluster_id] = (centroid_global_index, distance_matrix)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66a96e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_approximated_distance_matrix_optimized(dataframes: List[pd.DataFrame],\n",
    "                                                   cluster_to_columns: Dict[int, List[int]], \n",
    "                                                   centroids: Dict[int, Tuple[int, np.ndarray]],\n",
    "                                                   s: int,\n",
    "                                                  index_to_tuple) -> np.ndarray:\n",
    "    \n",
    "    \"\"\"\n",
    "    Approximates general distance matrix\n",
    "    \n",
    "    Args:\n",
    "        dataframes (List[pd.DataFrame]): List of dataframes containing the data.\n",
    "        cluster_to_columns (Dict[int, List[int]]): Mapping of global cluster IDs to lists of global column indices.\n",
    "        s (int): sampling parameter.\n",
    "        index_to_tuple (Callable[[int], Tuple[int, int]]): global to local index map.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Approximated distance matrix. \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # initialize model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # create a dictionary to store the embeddings for each centroid column\n",
    "    centroid_embeddings = {}\n",
    "    \n",
    "    # get embeddings for all centroid columns\n",
    "    for cluster_id, (centroid_index, _) in centroids.items():\n",
    "        double_index = index_to_tuple(centroid_index)\n",
    "        centroid_column = dataframes[double_index[0]].iloc[:, double_index[1]]\n",
    "        \n",
    "        # sample s entries\n",
    "        filtered_column = centroid_column.dropna()\n",
    "        samples = filtered_column.sample(min(len(filtered_column), s)).astype(str)\n",
    "\n",
    "        centroid_embeddings[cluster_id] = np.mean(model.encode(samples.tolist()), axis=0)\n",
    "    \n",
    "    # prepare the embedding matrix for centroids\n",
    "    embedding_matrix = np.vstack(list(centroid_embeddings.values()))\n",
    "    \n",
    "    \n",
    "    # calculate cosine distance for centroids\n",
    "    centroid_distance_matrix = 1 - cosine_similarity(embedding_matrix)\n",
    "    \n",
    "    # initialization of approximated distance matrix\n",
    "    n = sum(df.shape[1] for df in dataframes)\n",
    "    approx_distance_matrix = np.zeros((n, n))\n",
    "    \n",
    "    # fill in the intra-cluster distances first\n",
    "    for cluster_id, (_, distance_matrix) in centroids.items():\n",
    "        cluster_columns = cluster_to_columns[cluster_id]\n",
    "        approx_distance_matrix[np.ix_(cluster_columns, cluster_columns)] = distance_matrix\n",
    "    \n",
    "    # now handle the inter-cluster distances using the centroid distances\n",
    "    for cluster_id1, cluster_id2 in itertools.combinations(centroids.keys(), 2):\n",
    "        cluster_columns1 = cluster_to_columns[cluster_id1]\n",
    "        cluster_columns2 = cluster_to_columns[cluster_id2]\n",
    "\n",
    "        distance_between_centroids = centroid_distance_matrix[cluster_id1, cluster_id2]\n",
    "\n",
    "        approx_distance_matrix[np.ix_(cluster_columns1, cluster_columns2)] = distance_between_centroids\n",
    "        approx_distance_matrix[np.ix_(cluster_columns2, cluster_columns1)] = distance_between_centroids\n",
    "    \n",
    "    return approx_distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6659ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_script() -> dict:\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # path to the folder containing CSV files \n",
    "    base_dir = os.path.dirname(os.path.abspath(\"Baseline.ipynb\"))\n",
    "    folder_path = os.path.join(base_dir, \"..\",\"Data\", \"GitTables\", \"tables\")\n",
    "\n",
    "    # maximum number of files to be read\n",
    "    file_limit = 1000\n",
    "\n",
    "    # read dataframes from the folder\n",
    "    tuple_dataframes = read_dataframes_from_folder(folder_path, file_limit)\n",
    "\n",
    "    # reading in true labels for evaluation\n",
    "    base_dir = os.path.dirname(os.path.abspath(\"Baseline.ipynb\"))\n",
    "    label_path = os.path.join(base_dir, \"..\", \"Data\", \"GitTables\", \"dbpedia_gt.csv\")\n",
    "    label_df = pd.read_csv(label_path)\n",
    "    \n",
    "    # clean the dataset and drop columns without gold standard annotations\n",
    "    clean_dataframes(tuple_dataframes)\n",
    "    tuple_dataframes_copy = drop_columns_without_label(tuple_dataframes, label_df)\n",
    "\n",
    "    random_sample = sample_columns(tuple_dataframes_copy, 2000)\n",
    "\n",
    "    ground_truth_map = create_ground_truth_map(label_df, random_sample)\n",
    "    \n",
    "    dataframes = [df for df, _ in random_sample]\n",
    "\n",
    "    \n",
    "    index_lookup = {}\n",
    "    count = 0\n",
    "    for i in range(len(dataframes)):\n",
    "        for j in range(len(dataframes[i].columns)):\n",
    "            index_lookup[count] = (i, j)\n",
    "            count += 1\n",
    "\n",
    "    def index_to_tuple(index):\n",
    "        return index_lookup[index]\n",
    "\n",
    "    # execute SemJET\n",
    "\n",
    "    s = 12\n",
    "    num_pre_cluster = 60\n",
    "    \n",
    "    f = extract_features(dataframes)\n",
    "    f_normalized = z_normalize_features(f.fillna(0))\n",
    "    \n",
    "    pre_cl = pre_clustering(f_normalized.fillna(0), num_pre_cluster)\n",
    "    cluster_centroids = compute_cluster_centroids(dataframes, pre_cl, s, index_to_tuple)\n",
    "    approx_d = compute_approximated_distance_matrix_optimized(dataframes, pre_cl, cluster_centroids, s, index_to_tuple)                 \n",
    "    \n",
    "    \n",
    "    # clustering hierarchically\n",
    "    num_hierarchical_clusters = len(ground_truth_map.keys())\n",
    "\n",
    "    cl = hierarchical_clustering(approx_d, num_hierarchical_clusters)\n",
    "    \n",
    "    \n",
    "    # evaluate clustering performance\n",
    "    precision, recall = evaluate_micro(ground_truth_map, cl)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall'] = recall\n",
    "    metrics['f1_score'] = 2*(precision*recall)/(precision + recall)\n",
    "    metrics['calculated_embeddings'] = s*(0.5 * num_pre_cluster*(1-num_pre_cluster) \n",
    "                                          + sum(0.5 * len(value) * (len(value) - 1) for value in pre_cl.values()))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace3d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cfdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d8aaefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.5674710390754522, 'recall': 0.5166163141993958, 'f1_score': 0.5408508746762136, 'calculated_embeddings': 12857916.0}\n"
     ]
    }
   ],
   "source": [
    "# ADJUST ACCORDING TO PREFERENCE\n",
    "# FOR STABLE RESULT CONISDER num_iteraitons = 10,\n",
    "# FOR QUICK EXECUTION num_itertaions = 1\n",
    "num_iterations = 1\n",
    "\n",
    "results = [execute_script() for _ in range(num_iterations)]\n",
    "averages = {key: sum([result[key] for result in results]) / num_iterations for key in results[0]}\n",
    "print(averages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
