{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606160b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Set, Tuple, List\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fab97624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframes_from_folder(folder_path: str, file_limit: int, start_index: int = 0) -> List[Tuple[pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Read dataframes from CSV files in the given folder starting from the given index.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        file_limit (int): Maximum number of files to be read.\n",
    "        start_index (int, optional): The index from which to start reading the files. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[pd.DataFrame, str]]: A list of tuples containing the dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    tuple_dataframes = []\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(folder_path)):\n",
    "        if idx < start_index:\n",
    "            continue\n",
    "        \n",
    "        if filename.endswith('.csv'):\n",
    "            if len(tuple_dataframes) < file_limit:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                df = pd.read_csv(file_path)\n",
    "                tuple_dataframes.append((df, os.path.basename(file_path)))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return tuple_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bccdf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def clean_dataframes(tuple_dataframes: List[Tuple[pd.DataFrame, str]]) -> None:\n",
    "    \"\"\"\n",
    "    Clean the dataframes by renaming columns, dropping the 'Unnamed: 0' column, and dropping empty columns.\n",
    "\n",
    "    Args:\n",
    "        tuple_dataframes (List[Tuple[pd.DataFrame, str]]): List of tuples containing the dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to drop empty columns from dataframes\n",
    "    def drop_empty_columns_in_dataframes(dataframes: List[Tuple[pd.DataFrame, str]]) -> None:\n",
    "        for df, _ in dataframes:\n",
    "            empty_columns = df.columns[df.isnull().all()].tolist()  # get the list of empty columns\n",
    "            df.drop(empty_columns, axis=1, inplace=True)  # drop the empty columns\n",
    "    \n",
    "    # Renaming columns, deleting 'Unnamed: 0', and removing empty dataframes\n",
    "    indices_to_delete = []\n",
    "    for index, (df, _) in enumerate(tuple_dataframes):\n",
    "        df.rename(columns=lambda x: x.replace('col', ''), inplace=True)\n",
    "        \n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df.drop('Unnamed: 0', axis=1, inplace=True)  # delete the \"Unnamed: 0\" column\n",
    "        \n",
    "        if df.empty:\n",
    "            indices_to_delete.append(index)  # mark dataframe for deletion if empty\n",
    "    \n",
    "    # Reverse sort the indices and delete the empty dataframes\n",
    "    for index in sorted(indices_to_delete, reverse=True):\n",
    "        del tuple_dataframes[index]\n",
    "\n",
    "    # Call the helper function to drop empty columns\n",
    "    drop_empty_columns_in_dataframes(tuple_dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce4f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_without_label(dataframes: List[Tuple[pd.DataFrame, str]], label_df: pd.DataFrame) -> List[Tuple[pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Drop columns for which no gold standard exists from the given list of dataframes.\n",
    "\n",
    "    Args:\n",
    "        dataframes (List[Tuple[pd.DataFrame, str]]): List of tuples containing the dataframes and their corresponding filenames.\n",
    "        label_df (pd.DataFrame): DataFrame containing the gold standard annotations.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[pd.DataFrame, str]]: List of tuples containing the cleaned dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the input dataframes\n",
    "    tuple_dataframes_copy = [(df.copy(), name) for df, name in dataframes]\n",
    "\n",
    "    # Indices of dataframes to delete\n",
    "    to_delete = []\n",
    "\n",
    "    # Iterate over dataframes\n",
    "    for table_index, (df, filename) in enumerate(tuple_dataframes_copy):\n",
    "        for column_index, col in enumerate(df.columns):\n",
    "            if not any((label_df['table_id'] == filename[:-4] + \"_dbpedia\") & (label_df['target_column'] == column_index)):\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "        if df.empty:\n",
    "            to_delete.append(table_index)\n",
    "        else:\n",
    "            tuple_dataframes_copy[table_index] = (df, filename)\n",
    "\n",
    "    # Delete empty dataframes from the list in reverse order to prevent index shifting\n",
    "    for index in reversed(to_delete):\n",
    "        del tuple_dataframes_copy[index]\n",
    "\n",
    "    # Print the total number of remaining columns\n",
    "    print(f\"Total number of remaining columns : {sum([len(df.columns) for df, _ in tuple_dataframes_copy])}\")\n",
    "\n",
    "    return tuple_dataframes_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "250afd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_map(label_df: pd.DataFrame, tuple_dataframes: List[Tuple[pd.DataFrame, str]]) -> Dict[str, Set[int]]:\n",
    "    \"\"\"\n",
    "    Create a ground truth map for evaluation.\n",
    "\n",
    "    Args:\n",
    "        label_df (pd.DataFrame): DataFrame containing true labels for evaluation.\n",
    "        tuple_dataframes (List[Tuple[pd.DataFrame, str]]): List of tuples containing the dataframes and their corresponding filenames.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Set[int]]: A dictionary mapping label names to sets of global indices that belong to each label.\n",
    "    \"\"\"\n",
    "    ground_truth = defaultdict(set)\n",
    "\n",
    "    for table_index, (df, filename) in enumerate(tuple_dataframes):\n",
    "        for new_col_index, old_col_index in enumerate(df.columns):\n",
    "            label = label_df.query(f\"table_id == '{filename[:-4] + '_dbpedia'}' and target_column == {old_col_index}\")['annotation_label']\n",
    "\n",
    "            if label.empty:\n",
    "                continue\n",
    "\n",
    "            global_index = sum(df.shape[1] for df, _ in tuple_dataframes[:table_index]) + new_col_index\n",
    "            ground_truth[label.iloc[0]].add(global_index)\n",
    "\n",
    "            if len(label.values) != 1:\n",
    "                raise ValueError(\"Unexpected number of label values\")\n",
    "\n",
    "    for tuple_dataframe in tuple_dataframes:\n",
    "        tuple_dataframe[0].columns = range(len(tuple_dataframe[0].columns))\n",
    "\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3a80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustering(distance_matrix: np.ndarray, n_clusters: int) -> Dict[int, Set[int]]:\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on the distance matrix.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (np.ndarray): Pairwise distance matrix.\n",
    "        n_clusters (int): Number of clusters to create.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Set[int]]: A dictionary mapping cluster labels to sets of indices that belong to each cluster.\n",
    "    \"\"\"\n",
    "    # create an AgglomerativeClustering model\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage='average')\n",
    "\n",
    "    # fit the model to the distance matrix\n",
    "    model.fit(distance_matrix)\n",
    "\n",
    "    # get the cluster labels\n",
    "    labels = model.labels_\n",
    "\n",
    "    # create a dictionary that maps each cluster label to the set of indices that belong to that cluster\n",
    "    clusters = defaultdict(set)\n",
    "    for idx, label in enumerate(labels):\n",
    "        clusters[label].add(idx)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd635296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_micro(truth_dict: Dict[str, Set[int]], result_dict: Dict[int, Set[int]]) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance using micro-average precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        truth_dict (Dict[str, Set[int]]): Ground truth mapping label names to sets of global indices.\n",
    "        result_dict (Dict[int, Set[int]]): Resulting clustering mapping cluster labels to sets of indices.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Micro-average precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    # Creating the contingency matrix\n",
    "    contingency_matrix = np.zeros((len(truth_dict), len(result_dict)))\n",
    "\n",
    "    truth_labels = list(truth_dict.keys())\n",
    "    result_labels = list(result_dict.keys())\n",
    "\n",
    "    for i, truth_label in enumerate(truth_labels):\n",
    "        for j, result_label in enumerate(result_labels):\n",
    "            # Length of intersection between clusters\n",
    "            # contingency_matrix[i, j] is the number of (global) indices assigned to both clusters\n",
    "            contingency_matrix[i, j] = len(truth_dict[truth_label] & result_dict[result_label])\n",
    "\n",
    "    # Calculate precision, recall, and f1-score for each label and average them\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for i, _ in enumerate(result_labels):\n",
    "        tp = np.max(contingency_matrix[:, i])  # Maximum element in column i (True Positives)\n",
    "        best_match_index = np.argmax(contingency_matrix[:, i])  # Get the index of the best match (TP - index)\n",
    "        tp_plus_fp = np.sum(contingency_matrix[best_match_index, :])  # Sum of elements in row of best match (TP + FP)\n",
    "        tp_plus_fn = np.sum(contingency_matrix[:, i])  # Sum of elements in column i (TP + FN)\n",
    "\n",
    "        weight = tp_plus_fn  # The weight for each label is the total number of true positives plus false negatives\n",
    "\n",
    "        if tp_plus_fp > 0:\n",
    "            precision = tp / tp_plus_fp\n",
    "            total_precision += weight * precision\n",
    "        if tp_plus_fn > 0:\n",
    "            recall = tp / tp_plus_fn\n",
    "            total_recall += weight * recall\n",
    "\n",
    "        total_weight += weight\n",
    "\n",
    "    avg_precision = total_precision / total_weight\n",
    "    avg_recall = total_recall / total_weight\n",
    "\n",
    "    return avg_precision, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a04c27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_columns(tuple_dataframes: List[Tuple[pd.DataFrame, str]], n: int) -> List[Tuple[pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Sample 'n' columns from all DataFrames combined, delete non-sampled columns, and filter out empty DataFrames.\n",
    "\n",
    "    Args:\n",
    "        tuple_dataframes (List[Tuple[pd.DataFrame, str]]): A list of tuples containing the dataframes and their corresponding filenames.\n",
    "        n (int): Number of columns to sample in total.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[pd.DataFrame, str]]: A list of tuples containing the non-empty dataframes and their corresponding filenames.\n",
    "    \"\"\"\n",
    "    # Add prefixes to ensure unique column names in combined DataFrame\n",
    "    all_dfs = []\n",
    "    for idx, (df, _) in enumerate(tuple_dataframes):\n",
    "        df_prefixed = df.add_prefix(f\"df{idx}_\")\n",
    "        all_dfs.append(df_prefixed)\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, axis=1)\n",
    "\n",
    "    # Sample 'n' columns from the combined DataFrame\n",
    "    if n >= len(combined_df.columns):\n",
    "        sampled_columns = combined_df.columns\n",
    "    else:\n",
    "        sampled_columns = random.sample(list(combined_df.columns), n)\n",
    "\n",
    "    # Filter combined DataFrame to keep only the sampled columns\n",
    "    sampled_combined_df = combined_df[sampled_columns]\n",
    "\n",
    "    filtered_dataframes = []\n",
    "\n",
    "    # Split the combined DataFrame back into individual DataFrames and filter out any empty DataFrames\n",
    "    for idx, (_, filename) in enumerate(tuple_dataframes):\n",
    "        relevant_cols = [col for col in sampled_combined_df.columns if col.startswith(f\"df{idx}_\")]\n",
    "        df_sampled = sampled_combined_df[relevant_cols].rename(columns=lambda x: x.split(\"_\", 1)[1])\n",
    "        if not df_sampled.empty:\n",
    "            filtered_dataframes.append((df_sampled, filename))\n",
    "\n",
    "    return filtered_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f126a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(emb1, emb2):\n",
    "    # Cosine similarity ranges from -1 (exactly opposite) to 1 (exactly the same), \n",
    "    # with 0 indicating orthogonality (decorrelation). \n",
    "    # Since we need a distance, which is positive and larger for dissimilar items, \n",
    "    # subtract cosine similarity from 1.\n",
    "    return 1 - np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faf376ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Number of Entries\n",
    "def number_of_entries(column: pd.Series) -> int:\n",
    "    return len(column)\n",
    "\n",
    "# 2. Maximum Entry Length\n",
    "def max_entry_length(column: pd.Series) -> int:\n",
    "    return column.astype(str).str.len().max()\n",
    "\n",
    "# 3. Average Alphabetic Characters per Entry\n",
    "def avg_alpha_characters(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: sum(c.isalpha() for c in str(x))).mean()\n",
    "\n",
    "# 4. Proportion of Entries Containing Numbers\n",
    "def proportion_of_numeric_entries(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: str(x).replace(\".\", \"\", 1).isdigit()).mean()\n",
    "\n",
    "# 5. Column Entropy\n",
    "def column_entropy(column: pd.Series) -> float:\n",
    "    value_counts = column.value_counts(normalize=True)\n",
    "    return -np.sum(value_counts * np.log2(value_counts))\n",
    "\n",
    "# 6. Proportion of Entries with Letters\n",
    "def proportion_of_entries_with_letters(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: any(c.isalpha() for c in x)).mean()\n",
    "\n",
    "# 7. Number of Empty Entries\n",
    "def number_of_empty_entries(column: pd.Series) -> int:\n",
    "    return column.isna().sum()\n",
    "\n",
    "# 8. Average Length of Entries\n",
    "def average_length_of_entries(column: pd.Series) -> float:\n",
    "    return column.astype(str).str.len().mean()\n",
    "\n",
    "# 9. Proportion of Distinct Values\n",
    "def ratio_of_unique_entries(column: pd.Series) -> float:\n",
    "    return column.nunique() / len(column)\n",
    "\n",
    "# 10. Average Number of Numerical Characters per Entry\n",
    "def avg_number_of_numerical_characters(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: sum(c.isdigit() for c in str(x))).mean()\n",
    "\n",
    "# 11-13. Mean, Median and Standard Deviation of Numeric Entries\n",
    "def get_numeric_entries(column: pd.Series) -> pd.Series:\n",
    "    numeric_entries = column.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    return numeric_entries[numeric_entries.notna()]\n",
    "\n",
    "def mean_of_numeric_entries(column: pd.Series) -> float:\n",
    "    numeric_entries = get_numeric_entries(column)\n",
    "    return numeric_entries.mean()\n",
    "\n",
    "def median_of_numeric_entries(column: pd.Series) -> float:\n",
    "    numeric_entries = get_numeric_entries(column)\n",
    "    return numeric_entries.median()\n",
    "\n",
    "def std_dev_of_numeric_entries(column: pd.Series) -> float:\n",
    "    numeric_entries = get_numeric_entries(column)\n",
    "    return numeric_entries.std()\n",
    "\n",
    "# 14. Proportion of Alphanumerical Entries\n",
    "def proportion_of_alphanumeric_entries(column: pd.Series) -> float:\n",
    "    alphanumeric_pattern = re.compile(r'\\w')\n",
    "    return column.astype(str).apply(lambda x: bool(alphanumeric_pattern.match(x))).mean()\n",
    "\n",
    "# 15. Proportion of Numeric-only Entries\n",
    "def proportion_of_integer_entries(column: pd.Series) -> float:\n",
    "    return column.apply(lambda x: str(x).isdigit()).mean()\n",
    "\n",
    "# 16. Proportion of Data Entries\n",
    "def proportion_of_date_entries(column: pd.Series) -> float:\n",
    "    date_pattern = re.compile(r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})\\b')\n",
    "    return column.astype(str).apply(lambda x: bool(date_pattern.match(x))).mean()\n",
    "\n",
    "# 17. Proportion of Entries with Special Characters\n",
    "def proportion_of_entries_with_special_characters(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: any(not c.isalnum() and not c.isspace() for c in x)).mean()\n",
    "\n",
    "# 18. Proportion of Entries Containing Spaces\n",
    "def proportion_of_entries_with_spaces(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: ' ' in x).mean()\n",
    "\n",
    "# 19. Proportion of Link Entries\n",
    "def proportion_of_link_entries(column: pd.Series) -> float:\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return column.astype(str).apply(lambda x: bool(url_pattern.search(x))).mean()\n",
    "\n",
    "# 20. Proportion of Entries Initiating with Capital Letters\n",
    "def proportion_of_capitalized_entries(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: x[0].isupper() if x else False).mean()\n",
    "\n",
    "# 21. Proportion of Entries in Uppercase\n",
    "def proportion_of_fully_capitalized_entries(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: x.isupper()).mean()\n",
    "\n",
    "# 22. Proportion of Words Starting with a Capital Letter\n",
    "def proportion_of_capitalized_words(column: pd.Series) -> float:\n",
    "    return column.astype(str).apply(lambda x: sum(1 for word in x.split() if word.istitle())).mean()\n",
    "\n",
    "feature_extractors = {\n",
    "    \"Number of Entries\": number_of_entries,\n",
    "    \"Maximum Entry Length\": max_entry_length,\n",
    "    \"Average Alphabetic Characters per Entry\": avg_alpha_characters,\n",
    "    \"Proportion of Entries Containing Numbers\": proportion_of_numeric_entries,\n",
    "    \"Column Entropy\": column_entropy,\n",
    "    \"Proportion of Entries with Letters\": proportion_of_entries_with_letters,\n",
    "    \"Number of Empty Entries\": number_of_empty_entries,\n",
    "    \"Average Length of Entries\": average_length_of_entries,\n",
    "    \"Proportion of Distinct Values\": ratio_of_unique_entries,\n",
    "    \"Average Number of Numerical Characters per Entry\": avg_number_of_numerical_characters,\n",
    "    \"Mean Numeric Value\": mean_of_numeric_entries,\n",
    "    \"Median of Numeric Values\": median_of_numeric_entries,\n",
    "    \"Spread of Numeric Values (Standard Deviation)\": std_dev_of_numeric_entries,\n",
    "    \"Proportion of Alphanumerical Entries\": proportion_of_alphanumeric_entries,\n",
    "    \"Proportion of Numeric-only Entries\": proportion_of_integer_entries,\n",
    "    \"Proportion of Data Entries\": proportion_of_date_entries,\n",
    "    \"Proportion of Entries with Special Characters\": proportion_of_entries_with_special_characters,\n",
    "    \"Proportion of Entries Containing Spaces\": proportion_of_entries_with_spaces,\n",
    "    \"Proportion of Link Entries\": proportion_of_link_entries,\n",
    "    \"Proportion of Entries Initiating with Capital Letters\": proportion_of_capitalized_entries,\n",
    "    \"Proportion of Entries in Uppercase\": proportion_of_fully_capitalized_entries,\n",
    "    \"Proportion of Words Starting with a Capital Letter\": proportion_of_capitalized_words\n",
    "}\n",
    "\n",
    "def extract_features(dataframes: list) -> pd.DataFrame:\n",
    "    \n",
    "    # initialize an empty DataFrame to store the features\n",
    "    features = pd.DataFrame(columns=feature_extractors.keys())\n",
    "    \n",
    "    for df in dataframes:\n",
    "        \n",
    "        # dataframe for features of current df\n",
    "        temp_features = pd.DataFrame(columns=feature_extractors.keys())  \n",
    "        \n",
    "        for column_name, column_data in df.items():\n",
    "            \n",
    "            # convert column_data to a pd.Series of strings\n",
    "            column_data_series = pd.Series(map(str, column_data))\n",
    "            \n",
    "            for feature_name, feature_extractor in feature_extractors.items():\n",
    "                \n",
    "                # loc at (column_name, feature_name)\n",
    "                temp_features.loc[column_name, feature_name] = feature_extractor(column_data_series)\n",
    "                \n",
    "        # rowwise concatenation        \n",
    "        features = pd.concat([features, temp_features]) \n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83264145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_normalize_features(feature_set):\n",
    "    # Compute the mean and standard deviation for each feature (column).\n",
    "    means = np.mean(feature_set, axis=0)\n",
    "    stds = np.std(feature_set, axis=0)\n",
    "\n",
    "    # Z-normalize each feature in the feature set.\n",
    "    z_normalized_features = (feature_set - means) / stds\n",
    "\n",
    "    return z_normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d6ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_clustering(feature_df: pd.DataFrame, n_clusters: int) -> Dict[int, List[int]]:\n",
    "    \n",
    "    # initializing Birch and fit it to the data\n",
    "    birch_model = Birch(n_clusters=n_clusters)\n",
    "    birch_model.fit(feature_df)\n",
    "\n",
    "    # get the cluster labels\n",
    "    labels = birch_model.labels_\n",
    "\n",
    "    # create a dictionary that maps each cluster label to a list of column indices\n",
    "    cluster_to_columns = defaultdict(list)\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        cluster_to_columns[label].append(idx) # globale index\n",
    "\n",
    "    return dict(cluster_to_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7bafd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_column_similarity(cluster: List[List], s) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the similarity between columns of data using Sentence BERT embeddings.\n",
    "    \"\"\"\n",
    "    # Convert cluster columns to dataframes for uniform processing\n",
    "    dfs = [pd.DataFrame(col) for col in cluster]\n",
    "    \n",
    "    column_embeddings = {}\n",
    "    count = 0\n",
    "    for df in dfs:\n",
    "        for col in df.columns:\n",
    "            # Sample a subset of entries from the column\n",
    "            samples = df[col].dropna().astype(str)\n",
    "            samples = samples.sample(min(len(samples), s))\n",
    "\n",
    "            # Embed\n",
    "            sample_embeddings = model.encode(samples.tolist())\n",
    "            column_embeddings[count] = np.mean(sample_embeddings, axis=0)\n",
    "            count += 1\n",
    "\n",
    "    # Compute the distance\n",
    "    embedding_matrix = np.vstack(list(column_embeddings.values()))\n",
    "    distance = 1 - cosine_similarity(embedding_matrix)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def compute_distance_matrix(cluster: List[List], s) -> np.ndarray:\n",
    "    return calculate_column_similarity(cluster, s)\n",
    "        \n",
    "\n",
    "def find_centroid(cluster: List[List], s) -> Tuple[int, np.ndarray]:\n",
    "    distance_matrix = compute_distance_matrix(cluster, s)\n",
    "    centroid_intern_index = np.argmin(np.sum(distance_matrix, axis=0))\n",
    "    return centroid_intern_index, distance_matrix\n",
    "\n",
    "def compute_cluster_centroids(dataframes: List[pd.DataFrame],\n",
    "                              cluster_to_columns: Dict[int, List[int]],\n",
    "                              s: int,\n",
    "                             index_to_tuple) -> Dict[int, Tuple[int, np.ndarray]]:\n",
    "    centroids = {}\n",
    "\n",
    "    for cluster_id, column_indices in cluster_to_columns.items():\n",
    "        cluster_columns = [dataframes[df_index].iloc[:, col_index].tolist() for df_index, col_index in map(index_to_tuple, column_indices)]\n",
    "        centroid_intern_index, distance_matrix = find_centroid(cluster_columns, s)\n",
    "        centroid_global_index = column_indices[centroid_intern_index]\n",
    "        centroids[cluster_id] = (centroid_global_index, distance_matrix)\n",
    "\n",
    "    return centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a96e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_approximated_distance_matrix_optimized(dataframes: List[pd.DataFrame],\n",
    "                                                   cluster_to_columns: Dict[int, List[int]], \n",
    "                                                   centroids: Dict[int, Tuple[int, np.ndarray]],\n",
    "                                                   s: int,\n",
    "                                                  index_to_tuple) -> np.ndarray:\n",
    "\n",
    "    \n",
    "    # Initialize the model once\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Create a dictionary to store the embeddings for each centroid column\n",
    "    centroid_embeddings = {}\n",
    "    \n",
    "    # Get embeddings for all centroid columns\n",
    "    for cluster_id, (centroid_index, _) in centroids.items():\n",
    "        double_index = index_to_tuple(centroid_index)\n",
    "        centroid_column = dataframes[double_index[0]].iloc[:, double_index[1]]\n",
    "        \n",
    "        # Sample s entries\n",
    "        filtered_column = centroid_column.dropna()\n",
    "        samples = filtered_column.sample(min(len(filtered_column), s)).astype(str)\n",
    "\n",
    "        centroid_embeddings[cluster_id] = np.mean(model.encode(samples.tolist()), axis=0)\n",
    "    \n",
    "    # Prepare the embedding matrix for centroids\n",
    "    embedding_matrix = np.vstack(list(centroid_embeddings.values()))\n",
    "    \n",
    "    \n",
    "    # Calculate cosine distance for centroids\n",
    "    centroid_distance_matrix = 1 - cosine_similarity(embedding_matrix)\n",
    "    \n",
    "    # initialization of approximated distance matrix\n",
    "    n = sum(df.shape[1] for df in dataframes)\n",
    "    approx_distance_matrix = np.zeros((n, n))\n",
    "    \n",
    "    # Fill in the intra-cluster distances first\n",
    "    for cluster_id, (_, distance_matrix) in centroids.items():\n",
    "        cluster_columns = cluster_to_columns[cluster_id]\n",
    "        approx_distance_matrix[np.ix_(cluster_columns, cluster_columns)] = distance_matrix\n",
    "    \n",
    "    # Now, handle the inter-cluster distances using the centroid distances\n",
    "    for cluster_id1, cluster_id2 in itertools.combinations(centroids.keys(), 2):\n",
    "        cluster_columns1 = cluster_to_columns[cluster_id1]\n",
    "        cluster_columns2 = cluster_to_columns[cluster_id2]\n",
    "\n",
    "        distance_between_centroids = centroid_distance_matrix[cluster_id1, cluster_id2]\n",
    "\n",
    "        approx_distance_matrix[np.ix_(cluster_columns1, cluster_columns2)] = distance_between_centroids\n",
    "        approx_distance_matrix[np.ix_(cluster_columns2, cluster_columns1)] = distance_between_centroids\n",
    "    \n",
    "    return approx_distance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6659ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_script() -> dict:\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "   # Path to the folder containing CSV files (needs to be adjusted accordingly)\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    folder_path = os.path.join(base_dir, \"Data\", \"GitTables\", \"tables\")\n",
    "\n",
    "    # Maximum number of files to be read\n",
    "    file_limit = 1000\n",
    "\n",
    "    # Read dataframes from the folder\n",
    "    tuple_dataframes = read_dataframes_from_folder(folder_path, file_limit)\n",
    "\n",
    "    # Print the total number of read-in files and columns\n",
    "    total_columns = sum(df.shape[1] for df, _ in tuple_dataframes)\n",
    "    #print(f\"Total number of read-in files: {len(tuple_dataframes)} with a total of {total_columns} columns.\")\n",
    "\n",
    "    # Reading in true labels for evaluation (needs to be adjusted accordingly)\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    label_path = os.path.join(base_dir, \"Data\", \"GitTables\", \"dbpedia_gt.csv\")\n",
    "    label_df = pd.read_csv(label_path)\n",
    "\n",
    "    clean_dataframes(tuple_dataframes)\n",
    "    # Clean the dataset and drop columns without gold standard annotations\n",
    "    tuple_dataframes_copy = drop_columns_without_label(tuple_dataframes, label_df)\n",
    "    #print(f\"Total number of remaining columns after deletion of columns with no gold standard: {sum([len(df.columns) for df, _ in tuple_dataframes_copy])}\")\n",
    "\n",
    "    random_sample = sample_columns(tuple_dataframes_copy, 2000)\n",
    "\n",
    "    ground_truth_map = create_ground_truth_map(label_df, random_sample)\n",
    "\n",
    "    # Extract dataframes from the tuple list\n",
    "    dataframes = [df for df, _ in random_sample]\n",
    "    print(f\"Remaining columns after sampling: {sum([len(df.columns) for df in dataframes])}\")\n",
    "\n",
    "    \n",
    "    index_lookup = {}\n",
    "    count = 0\n",
    "    for i in range(len(dataframes)):\n",
    "        for j in range(len(dataframes[i].columns)):\n",
    "            index_lookup[count] = (i, j)\n",
    "            count += 1\n",
    "\n",
    "    def index_to_tuple(index):\n",
    "        return index_lookup[index]\n",
    "\n",
    "    # Execute SemJET\n",
    "\n",
    "    s = 12\n",
    "    num_pre_cluster = 60\n",
    "    \n",
    "    f = extract_features(dataframes)\n",
    "    f_normalized = z_normalize_features(f.fillna(0))\n",
    "    \n",
    "    pre_cl = pre_clustering(f_normalized.fillna(0), num_pre_cluster)\n",
    "    cluster_centroids = compute_cluster_centroids(dataframes, pre_cl, s, index_to_tuple)\n",
    "    approx_d = compute_approximated_distance_matrix_optimized(dataframes, pre_cl, cluster_centroids, s, index_to_tuple)                 \n",
    "    \n",
    "    \n",
    "    # Calculate the number of hierarchical clusters\n",
    "    num_hierarchical_clusters = len(ground_truth_map.keys())\n",
    "\n",
    "    cl = hierarchical_clustering(approx_d, num_hierarchical_clusters)\n",
    "    \n",
    "    \n",
    "    # Evaluate clustering performance\n",
    "    precision, recall = evaluate_micro(ground_truth_map, cl)\n",
    "    metrics['precision'] = precision\n",
    "    metrics['recall'] = recall\n",
    "    metrics['f1_score'] = 2*(precision*recall)/(precision + recall)\n",
    "    metrics['calculated_embeddings'] = s*(0.5 * num_pre_cluster*(1-num_pre_cluster) \n",
    "                                          + sum(0.5 * len(value) * (len(value) - 1) for value in pre_cl.values()))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace3d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caef0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8aaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [execute_script() for _ in range(10)]\n",
    "averages = {key: sum([result[key] for result in results]) / 10 for key in results[0]}\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62178c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c1186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9b3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS AS USED IN WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "83fc167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "Total number of remaining columns : 2178\n",
      "Remaining columns after sampling: 2000\n",
      "{'precision': 0.5388350520088381, 'recall': 0.5215489118795247, 'f1_score': 0.5266867462737947, 'calculated_embeddings': 13709954.4}\n"
     ]
    }
   ],
   "source": [
    "results = [execute_script() for _ in range(10)]\n",
    "averages = {key: sum([result[key] for result in results]) / 10 for key in results[0]}\n",
    "print(averages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
